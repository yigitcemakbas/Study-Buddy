{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers datasets sentence-transformers\n",
    "!pip install scikit_learn\n",
    "!pip install -q onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, sentence1_list, sentence2_list, labels, tokenizer, max_length=128):\n",
    "        self.sentence1 = sentence1_list\n",
    "        self.sentence2 = sentence2_list\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent1 = self.tokenizer(\n",
    "            self.sentence1[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        sent2 = self.tokenizer(\n",
    "            self.sentence2[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids_1': sent1['input_ids'].squeeze(0),\n",
    "            'attention_mask_1': sent1['attention_mask'].squeeze(0),\n",
    "            'input_ids_2': sent2['input_ids'].squeeze(0),\n",
    "            'attention_mask_2': sent2['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_dataset():\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "\n",
    "    sts = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "\n",
    "    cs_data = load_cs_dataset()  # We'll create this below\n",
    "\n",
    "    quora = load_dataset(\"quora\", split=\"train[:100000]\")  # 100K samples\n",
    "\n",
    "    all_sent1 = []\n",
    "    all_sent2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Process STS\n",
    "    for split in ['train', 'validation']:\n",
    "        for item in sts[split]:\n",
    "            all_sent1.append(item['sentence1'])\n",
    "            all_sent2.append(item['sentence2'])\n",
    "            all_labels.append(item['score'] / 5.0)  # Normalize to 0-1\n",
    "\n",
    "    # Process Quora\n",
    "    for item in quora:\n",
    "        all_sent1.append(item['questions']['text'][0])\n",
    "        all_sent2.append(item['questions']['text'][1])\n",
    "        all_labels.append(1.0 if item['is_duplicate'] else 0.0)\n",
    "\n",
    "    # Add CS data\n",
    "    all_sent1.extend(cs_data['sent1'])\n",
    "    all_sent2.extend(cs_data['sent2'])\n",
    "    all_labels.extend(cs_data['labels'])\n",
    "\n",
    "    print(f\"Total training pairs: {len(all_labels):,}\")\n",
    "\n",
    "    return all_sent1, all_sent2, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cs_dataset():\n",
    "    \"\"\"Generate comprehensive CS concept training pairs\"\"\"\n",
    "\n",
    "    concepts = {\n",
    "        # Programming Fundamentals\n",
    "        \"variable\": [\n",
    "            (\"What is a variable?\", \"A container that stores data values\", 1.0),\n",
    "            (\"What is a variable?\", \"A named memory location for storing values\", 0.95),\n",
    "            (\"What is a variable?\", \"Something that holds information in a program\", 0.85),\n",
    "            (\"What is a variable?\", \"A storage unit for data\", 0.9),\n",
    "            (\"Explain variable\", \"A container that stores data values\", 0.95),\n",
    "            (\"Define variable in programming\", \"A named memory location for storing values\", 0.9),\n",
    "            (\"What is a variable?\", \"A function\", 0.2),\n",
    "            (\"What is a variable?\", \"A loop\", 0.1),\n",
    "        ],\n",
    "\n",
    "        \"function\": [\n",
    "            (\"What is a function?\", \"A reusable block of code that performs a specific task\", 1.0),\n",
    "            (\"What is a function?\", \"A named section of code that can be called\", 0.95),\n",
    "            (\"What is a function?\", \"A way to organize code into reusable pieces\", 0.9),\n",
    "            (\"Explain function\", \"A reusable block of code that performs a specific task\", 0.95),\n",
    "            (\"What is a function?\", \"A variable\", 0.2),\n",
    "            (\"What is a function?\", \"A data type\", 0.1),\n",
    "        ],\n",
    "\n",
    "        \"loop\": [\n",
    "            (\"What is a for loop?\", \"A control structure that repeats code a specific number of times\", 1.0),\n",
    "            (\"What is a for loop?\", \"Iteration over a sequence of elements\", 0.9),\n",
    "            (\"What is a for loop?\", \"A way to repeat instructions multiple times\", 0.85),\n",
    "            (\"Explain for loop\", \"A control structure that repeats code a specific number of times\", 0.95),\n",
    "            (\"What is a for loop?\", \"A conditional statement\", 0.2),\n",
    "            (\"What is a for loop?\", \"A function\", 0.15),\n",
    "        ],\n",
    "\n",
    "        \"while_loop\": [\n",
    "            (\"What is a while loop?\", \"A loop that repeats as long as a condition is true\", 1.0),\n",
    "            (\"What is a while loop?\", \"A control structure that continues until a condition becomes false\", 0.95),\n",
    "            (\"What is a while loop?\", \"Iteration based on a boolean condition\", 0.9),\n",
    "            (\"What is a while loop?\", \"A for loop\", 0.3),\n",
    "        ],\n",
    "\n",
    "        # Data Structures\n",
    "        \"array\": [\n",
    "            (\"What is an array?\", \"A data structure that stores elements in contiguous memory\", 1.0),\n",
    "            (\"What is an array?\", \"A collection of elements of the same type\", 0.95),\n",
    "            (\"What is an array?\", \"An indexed list of items\", 0.9),\n",
    "            (\"What is an array?\", \"A fixed-size sequential collection\", 0.9),\n",
    "            (\"Explain array\", \"A data structure that stores elements in contiguous memory\", 0.95),\n",
    "            (\"What is an array?\", \"A variable\", 0.2),\n",
    "            (\"What is an array?\", \"A function\", 0.1),\n",
    "        ],\n",
    "\n",
    "        \"linked_list\": [\n",
    "            (\"What is a linked list?\", \"A linear data structure with nodes containing data and pointers\", 1.0),\n",
    "            (\"What is a linked list?\", \"A sequence where each element points to the next\", 0.95),\n",
    "            (\"What is a linked list?\", \"A chain of nodes connected by references\", 0.9),\n",
    "            (\"Explain linked list\", \"A linear data structure with nodes containing data and pointers\", 0.95),\n",
    "            (\"What is a linked list?\", \"An array\", 0.3),\n",
    "            (\"What is a linked list?\", \"A tree\", 0.4),\n",
    "        ],\n",
    "\n",
    "        \"stack\": [\n",
    "            (\"What is a stack?\", \"A LIFO data structure where elements are added and removed from the top\", 1.0),\n",
    "            (\"What is a stack?\", \"A last-in-first-out collection\", 0.95),\n",
    "            (\"What is a stack?\", \"A data structure with push and pop operations\", 0.9),\n",
    "            (\"Explain stack\", \"A LIFO data structure where elements are added and removed from the top\", 0.95),\n",
    "            (\"What is a stack?\", \"A queue\", 0.3),\n",
    "            (\"What is a stack?\", \"An array\", 0.4),\n",
    "        ],\n",
    "\n",
    "        \"queue\": [\n",
    "            (\"What is a queue?\", \"A FIFO data structure where elements are added at the rear and removed from the front\", 1.0),\n",
    "            (\"What is a queue?\", \"A first-in-first-out collection\", 0.95),\n",
    "            (\"What is a queue?\", \"A data structure with enqueue and dequeue operations\", 0.9),\n",
    "            (\"What is a queue?\", \"A stack\", 0.3),\n",
    "        ],\n",
    "\n",
    "        \"hash_table\": [\n",
    "            (\"What is a hash table?\", \"A data structure that maps keys to values using a hash function\", 1.0),\n",
    "            (\"What is a hash table?\", \"A collection using key-value pairs with O(1) lookup\", 0.95),\n",
    "            (\"What is a hash table?\", \"A dictionary implementation using hashing\", 0.9),\n",
    "            (\"Explain hash table\", \"A data structure that maps keys to values using a hash function\", 0.95),\n",
    "            (\"What is a hash table?\", \"An array\", 0.3),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    sent1, sent2, labels = [], [], []\n",
    "\n",
    "    for concept_name, pairs in concepts.items():\n",
    "        for s1, s2, label in pairs:\n",
    "            sent1.append(s1)\n",
    "            sent2.append(s2)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Generate additional negative pairs (unrelated concepts)\n",
    "    print(\"Generating negative pairs...\")\n",
    "    for _ in range(len(sent1)):\n",
    "        i, j = random.sample(range(len(sent1)), 2)\n",
    "        if labels[i] < 0.5 or labels[j] < 0.5:\n",
    "            continue\n",
    "        sent1.append(sent1[i])\n",
    "        sent2.append(sent2[j])\n",
    "        labels.append(0.1)\n",
    "\n",
    "\n",
    "    return sent1, sent2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_dataset():\n",
    "    \"\"\"Load and combine multiple datasets\"\"\"\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LOADING DATASETS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    all_sent1 = []\n",
    "    all_sent2 = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 1. CS-specific dataset (highest priority)\n",
    "    print(\"\\n1. Generating CS concept dataset...\")\n",
    "    cs_sent1, cs_sent2, cs_labels = generate_cs_dataset()\n",
    "\n",
    "    for _ in range(3):\n",
    "        all_sent1.extend(cs_sent1)\n",
    "        all_sent2.extend(cs_sent2)\n",
    "        all_labels.extend(cs_labels)\n",
    "\n",
    "    print(f\"   ✓ CS pairs (weighted 3x): {len(cs_sent1):,} → {len(cs_sent1)*3:,}\")\n",
    "\n",
    "    print(\"\\n2. Loading STS Benchmark...\")\n",
    "    try:\n",
    "        sts = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "        for split in ['train', 'validation']:\n",
    "            for item in sts[split]:\n",
    "                all_sent1.append(item['sentence1'])\n",
    "                all_sent2.append(item['sentence2'])\n",
    "                all_labels.append(item['score'] / 5.0)  # Normalize 0-5 to 0-1\n",
    "        print(f\"   ✓ STS pairs: {len(sts['train']) + len(sts['validation']):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Could not load STS: {e}\")\n",
    "\n",
    "    print(\"\\n3. Loading Quora dataset...\")\n",
    "    try:\n",
    "        quora = load_dataset(\"quora\", split=\"train[:50000]\")  # 50K samples\n",
    "        for item in quora:\n",
    "            if item['is_duplicate'] is not None:\n",
    "                all_sent1.append(item['questions']['text'][0])\n",
    "                all_sent2.append(item['questions']['text'][1])\n",
    "                all_labels.append(1.0 if item['is_duplicate'] else 0.0)\n",
    "        print(f\"   ✓ Quora pairs: {len(quora):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Could not load Quora: {e}\")\n",
    "\n",
    "    # 4. SNLI (Natural Language Inference)\n",
    "    print(\"\\n4. Loading SNLI dataset...\")\n",
    "    try:\n",
    "        snli = load_dataset(\"snli\", split=\"train[:30000]\")  # 30K samples\n",
    "        label_map = {'entailment': 0.9, 'neutral': 0.5, 'contradiction': 0.1}\n",
    "        for item in snli:\n",
    "            if item['label'] != -1:  # Skip unlabeled\n",
    "                label_text = ['entailment', 'neutral', 'contradiction'][item['label']]\n",
    "                all_sent1.append(item['premise'])\n",
    "                all_sent2.append(item['hypothesis'])\n",
    "                all_labels.append(label_map[label_text])\n",
    "        print(f\"   ✓ SNLI pairs: 30,000\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Could not load SNLI: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"TOTAL TRAINING PAIRS: {len(all_labels):,}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return all_sent1, all_sent2, all_labels\n",
    "\n",
    "print(\"Testing dataset loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseTransformer(nn.Module):\n",
    "    \"\"\"Siamese network with transformer encoder\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f\"Loading base model: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        # Projection head (map to shared embedding space)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "        # Similarity predictor\n",
    "        self.similarity_head = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        # Encode both sentences\n",
    "        outputs1 = self.encoder(input_ids=input_ids_1, attention_mask=attention_mask_1)\n",
    "        outputs2 = self.encoder(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
    "\n",
    "        # Use [CLS] token representation\n",
    "        emb1 = outputs1.last_hidden_state[:, 0, :]\n",
    "        emb2 = outputs2.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Project to shared space\n",
    "        proj1 = self.projection(emb1)\n",
    "        proj2 = self.projection(emb2)\n",
    "\n",
    "        # Concatenate and predict similarity\n",
    "        combined = torch.cat([proj1, proj2], dim=1)\n",
    "        similarity = self.similarity_head(combined)\n",
    "\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=100, batch_size=32, learning_rate=2e-5):\n",
    "    \"\"\"Train the siamese transformer\"\"\"\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Load data\n",
    "    sent1, sent2, labels = load_combined_dataset()\n",
    "\n",
    "    # Train/validation split\n",
    "    train_s1, val_s1, train_s2, val_s2, train_labels, val_labels = train_test_split(\n",
    "        sent1, sent2, labels, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining samples: {len(train_labels):,}\")\n",
    "    print(f\"Validation samples: {len(val_labels):,}\")\n",
    "\n",
    "    # Tokenizer\n",
    "    print(\"\\nLoading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"Creating dataloaders...\")\n",
    "    train_dataset = SentencePairDataset(train_s1, train_s2, train_labels, tokenizer)\n",
    "    val_dataset = SentencePairDataset(val_s1, val_s2, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = SiameseTransformer().to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2\n",
    "    )\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING PHASE\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "        for batch in train_bar:\n",
    "            input_ids_1 = batch['input_ids_1'].to(device)\n",
    "            attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "            input_ids_2 = batch['input_ids_2'].to(device)\n",
    "            attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "            labels_batch = batch['label'].to(device).unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # VALIDATION PHASE\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]  \")\n",
    "            for batch in val_bar:\n",
    "                input_ids_1 = batch['input_ids_1'].to(device)\n",
    "                attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "                input_ids_2 = batch['input_ids_2'].to(device)\n",
    "                attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "                labels_batch = batch['label'].to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                val_loss += loss.item()\n",
    "                val_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "        print(f\"  LR:         {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_siamese_model.pt')\n",
    "            print(f\"  ✓ New best model saved! (Val Loss: {best_val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{patience})\")\n",
    "\n",
    "        print(f\"{'─'*50}\\n\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⚠ Early stopping triggered at epoch {epoch+1}\")\n",
    "            print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training Progress', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training Progress (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Model saved as: best_siamese_model.pt\")\n",
    "    print(f\"Training curves saved as: training_curves.png\")\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = train_model(\n",
    "    num_epochs=2,\n",
    "    batch_size=32,\n",
    "    learning_rate=2e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving tokenizer configuration...\")\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "config = {\n",
    "    'vocab': vocab,\n",
    "    'max_length': 128,\n",
    "    'pad_token': tokenizer.pad_token,\n",
    "    'unk_token': tokenizer.unk_token,\n",
    "    'cls_token': tokenizer.cls_token,\n",
    "    'sep_token': tokenizer.sep_token,\n",
    "}\n",
    "\n",
    "with open('tokenizer_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Tokenizer config saved as: tokenizer_config.json\")\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_siamese_model.pt'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy inputs\n",
    "batch_size = 1\n",
    "seq_length = 128\n",
    "\n",
    "dummy_input_ids_1 = torch.randint(0, 30522, (batch_size, seq_length)).to(device)\n",
    "dummy_attention_mask_1 = torch.ones(batch_size, seq_length).to(device)\n",
    "dummy_input_ids_2 = torch.randint(0, 30522, (batch_size, seq_length)).to(device)\n",
    "dummy_attention_mask_2 = torch.ones(batch_size, seq_length).to(device)\n",
    "\n",
    "# Export\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input_ids_1, dummy_attention_mask_1, dummy_input_ids_2, dummy_attention_mask_2),\n",
    "    'siamese_transformer.onnx',\n",
    "    input_names=['input_ids_1', 'attention_mask_1', 'input_ids_2', 'attention_mask_2'],\n",
    "    output_names=['similarity_score'],\n",
    "    dynamic_axes={\n",
    "        'input_ids_1': {0: 'batch_size'},\n",
    "        'attention_mask_1': {0: 'batch_size'},\n",
    "        'input_ids_2': {0: 'batch_size'},\n",
    "        'attention_mask_2': {0: 'batch_size'},\n",
    "        'similarity_score': {0: 'batch_size'}\n",
    "    },\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "print(\"✓ Model exported as: siamese_transformer.onnx\")\n",
    "\n",
    "\n",
    "\n",
    "onnx_model = onnx.load('siamese_transformer.onnx')\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"✓ ONNX model verified successfully\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size_mb = os.path.getsize('siamese_transformer.onnx') / (1024 * 1024)\n",
    "print(f\"✓ Model size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing files for download...\")\n",
    "print(\"\\nFile sizes:\")\n",
    "print(f\"  siamese_transformer.onnx: {os.path.getsize('siamese_transformer.onnx')/(1024*1024):.2f} MB\")\n",
    "print(f\"  tokenizer_config.json: {os.path.getsize('tokenizer_config.json')/1024:.2f} KB\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DOWNLOAD THESE FILES:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. siamese_transformer.onnx\")\n",
    "print(\"2. tokenizer_config.json\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "files.download('siamese_transformer.onnx')\n",
    "files.download('tokenizer_config.json')\n",
    "\n",
    "\n",
    "print(\"\\nAll files ready for download.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
